---
title: "Stat 333 - Homework"
author: "Tess Steplyk"
date: "February 9, 2016"
output: html_document
---

###ISLR (p120)
\


####3.7.3

***Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get βˆ0 = 50, βˆ1 = 20 , βˆ2 = 0.07 , βˆ3 = 35 , βˆ4 = 0.01 , βˆ5 = −10.***
\
***(a) Which answer is correct, and why?
i. For a fixed value of IQ and GPA, males earn more on average than females.
\
ii. For a fixed value of IQ and GPA, females earn more on average than males.
\
iii. For a fixed value of IQ and GPA,males earn more on average than females provided that the GPA is high enough.
\
iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.***
\
(a) 
ŷ = 50 + 20(GPA) + 0.07(IQ) + 35(Gender) + 0.01(GPA×IQ) − 10(GPA×Gender)
\
(iii) is true. The higher a female's GPA is, the lower her earnings are in comparison to the male. On the other hand, at baseline levels of GPA/IQ, it seems that being a female earns higher than males.
\
***(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.***
\
Female: 
```{r}
50+(4.0*20)+(110*0.07)+35+0.01*110*4.0+(-10*4) 
```
Male: 
```{r}
50+(4.0*20)+(110*0.07)+0.01*110*4.0
```
\
***(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.***
\
False. Despite the small coefficient, I can't comment on the evidence of a lack of an interaction. For example it could be that the data has incredibly low variance around the fit, this would result in a highly confident interaction, even if it is small in magnitude.
False. To verify if the GPA/IQ has an impact on the quality of the model we need to test the hypothesis H0:β4^=0H0:β4^=0 and look at the p-value associated with the tt or the FF statistic to draw a conclusion.

\



\
####3.7.4

***I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 + β1X + β2X2 + β3X3 + ǫ.***
\
***(a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ǫ. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.***
\
Not enough information on the training data. If x is large, then the training RSS might be larger for cubic regression. If x is closer to zero, then the training RSS might be smaller for the cubic.
\
***(b) Answer (a) using test rather than training RSS.***
\
The test RSS depends upon the test data, and we don't have enough information to conclude. Might be able to guess that the cubic regression has a higher test RSS as the overfit from training would have more error than the linear regression.
\
***(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.***
\
Cubic regression has lower train RSS than the linear fit because the cubic is more flexible. More flexibility in means it will fit any observation in the training set much more closely than a more restrictive model.
\
***(d) Answer (c) using test rather than training RSS.***
\
We aren't given the underlying distribution so we don't know because if it's far from linear then a flexible model is best. If it's close to linear there's a chance a flexible model will still overfit the training observations and result in higher test RSS. So we just don't have enough information to tell which test will be lower.
\
####3.7.15
\
15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.
\
(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.
```{r,echo=FALSE}
library(MASS)
data(Boston)

lm.fit.zn=lm(crim~zn,data=Boston)
summary(lm.fit.zn)
lm.fit.indus=lm(crim~indus,data=Boston)
summary(lm.fit.indus)
lm.fit.chas=lm(crim~chas,data=Boston)
summary(lm.fit.chas)
lm.fit.nox=lm(crim~nox,data=Boston)
summary(lm.fit.nox)
lm.fit.rm=lm(crim~rm,data=Boston)
summary(lm.fit.rm)
lm.fit.age=lm(crim~age,data=Boston)
summary(lm.fit.age)
lm.fit.dis=lm(crim~dis,data=Boston)
summary(lm.fit.dis)
lm.fit.rad=lm(crim~rad,data=Boston)
summary(lm.fit.rad)
lm.fit.tax=lm(crim~tax,data=Boston)
summary(lm.fit.tax)
lm.fit.ptratio=lm(crim~ptratio,data=Boston)
summary(lm.fit.ptratio)
lm.fit.black=lm(crim~black,data=Boston)
summary(lm.fit.black)
lm.fit.lstat=lm(crim~lstat,data=Boston)
summary(lm.fit.lstat)
lm.fit.medv=lm(crim~medv,data=Boston)
summary(lm.fit.medv)

par(mfrow=c(3,4))
plot(Boston$zn, Boston$crim)
abline(lm.fit.zn, col="red",lwd=3)

plot(Boston$indus, Boston$crim)
abline(lm.fit.indus, col="red",lwd=3)

plot(Boston$nox, Boston$crim)
abline(lm.fit.nox, col="red",lwd=3)

plot(Boston$rm, Boston$crim)
abline(lm.fit.zn, col="red",lwd=3)

plot(Boston$age, Boston$crim)
abline(lm.fit.age, col="red",lwd=3)

plot(Boston$dis, Boston$crim)
abline(lm.fit.dis, col="red",lwd=3)

plot(Boston$rad, Boston$crim)
abline(lm.fit.rad, col="red",lwd=3)

plot(Boston$tax, Boston$crim)
abline(lm.fit.tax, col="red",lwd=3)

plot(Boston$ptratio, Boston$crim)
abline(lm.fit.ptratio, col="red",lwd=3)

plot(Boston$black, Boston$crim)
abline(lm.fit.black, col="red",lwd=3)

plot(Boston$lstat, Boston$crim)
abline(lm.fit.lstat, col="red",lwd=3)

plot(Boston$medv, Boston$crim)
abline(lm.fit.medv, col="red",lwd=3)

plot(Boston$chas, Boston$crim)
abline(lm.fit.chas, col="red",lwd=3)
```
\
To find the predictors that are significant, we test H0:β1=0H0:β1=0. Every predictor has a p-value > 0.05 except “chas”. We can conclude there's a statistically significant relationship between each predictor and the response except for the “chas” predictor.
\
\
(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?
```{r,echo=FALSE}
lm.fit.all=lm(crim~.,data=Boston)
summary(lm.fit.all)
```
Reject the null hypothesis for “rad”, “black”, “zn”, “dis”, and “medv”.
\
\
(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regres- sion model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.
```{r, echo=FALSE}
uni.coef <- c(coefficients(lm.fit.zn)[2],
              coefficients(lm.fit.indus)[2],
              coefficients(lm.fit.chas)[2],
              coefficients(lm.fit.nox)[2],
              coefficients(lm.fit.rm)[2],
              coefficients(lm.fit.age)[2],
              coefficients(lm.fit.dis)[2],
              coefficients(lm.fit.rad)[2],
              coefficients(lm.fit.tax)[2],
              coefficients(lm.fit.ptratio)[2],
              coefficients(lm.fit.black)[2],
              coefficients(lm.fit.lstat)[2],
              coefficients(lm.fit.medv)[2])
multi.coef <- coefficients(lm.fit.all)[-1] 

plot(uni.coef,multi.coef)
```

\
(d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form
Y = β0 +β1X +β2X2 +β3X3 +ǫ.
```{r, echo=FALSE}
lm.fit.zn=lm(crim~poly(zn,3),data=Boston)
summary(lm.fit.zn)
lm.fit.indus=lm(crim~poly(indus,3),data=Boston)
summary(lm.fit.indus)
lm.fit.nox=lm(crim~poly(nox,3),data=Boston)
summary(lm.fit.nox)
lm.fit.rm=lm(crim~poly(rm,3),data=Boston)
summary(lm.fit.rm)
lm.fit.age=lm(crim~poly(age,3),data=Boston)
summary(lm.fit.age)
lm.fit.dis=lm(crim~poly(dis,3),data=Boston)
summary(lm.fit.dis)
lm.fit.rad=lm(crim~poly(rad,3),data=Boston)
summary(lm.fit.rad)
lm.fit.tax=lm(crim~poly(tax,3),data=Boston)
summary(lm.fit.tax)
lm.fit.ptratio=lm(crim~poly(ptratio,3),data=Boston)
summary(lm.fit.ptratio)
lm.fit.black=lm(crim~poly(black,3),data=Boston)
summary(lm.fit.black)
lm.fit.lstat=lm(crim~poly(lstat,3),data=Boston)
summary(lm.fit.lstat)
lm.fit.medv=lm(crim~poly(medv,3),data=Boston)
summary(lm.fit.medv)
```
\
With “rm”, “rad”, “tax”, “zn”, and “lstat” as predictors, the p-values suggest the cubic coefficient isn't statistically significant. With “age”, “indus”, “nox”, “dis”, “ptratio”, and “medv” as predictors, the p-values suggest a good cubic fit. With “black” as a predictor, the p-values suggest that the cubic and the fourth power coefficients aren't statistically significant so in this case no non-linear effect is shown.